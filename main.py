import numpy as npimport theano.tensor as Timport mathclass Variational_Density_Estimation(object):    def __init__(self, fn, mu_init, var_init, learning_rate, no_of_sample, no_of_epochs, momentum=.9):        self.f = fn        self.mu_init = mu_init        self.var_init = var_init        self.learning_rate = learning_rate        self.n = no_of_sample        self.epoch = no_of_epochs        self.momentum = momentum    def grad(self, mu, sigma):        # w = T.vector('w')        # x = T.scalar('x')        # transformed_function = self.f(w[0] + w[1] * x)        x = T.scalar('x')        function_grad = T.grad(self.f(x), x)        sample = np.random.randn(self.n)        p_prime = np.array([function_grad.eval({x:mu+sigma*n}) for n in sample])        p = np.array([self.f(mu + sigma * n) for n in sample])        mu_grad = -np.mean(p_prime / p)        sigma_grad = -1 / sigma - np.mean((p_prime / p) * sample)        return mu_grad, sigma_grad    def get_param(self):        mu = self.mu_init        s = math.sqrt(self.var_init)        mu_arr = []        var_arr = []        mu_update = [0]        s_update = [0]        for i in range(self.epoch):            update1 = self.momentum * mu_update[i] + (1 - self.momentum) * self.grad(mu, s)[0]            update2 = self.momentum * mu_update[i] + (1 - self.momentum) * self.grad(mu, s)[1]            mu = mu - self.learning_rate * update1            s = s - self.learning_rate * update2            mu_update.append(update1)            s_update.append(update2)            # mu = mu - self.learning_rate*self.grad(mu, s)[0]            # s = s - self.learning_rate*self.grad(mu, s)[1]            mu_arr.append(mu)            var_arr.append(s ** 2)            # We want to print the loss after every 5 epochs            if i % 5 == 0:                sum = 0                for k in range(self.n):                    sample = mu + s * np.random.randn()                    sum = sum + np.log(self.f(sample))                loss = -math.log(s) - sum / self.n                print("Loss for epoch:{} is \t {:.4f}".format(i, loss))        return mu_arr, var_arrif __name__ == "__main__":    fn1 = lambda x: np.exp(-x ** 2 / (1.1 + np.sin(x)))    vi1 = Variational_Density_Estimation(fn1, 1, .1, .01, 1000, 200)    mu, var = vi1.get_param()